{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_on_CIFR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCBYFLVHICl4"
      },
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpTr48AGJDe0"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4bfAV91JGx6"
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 300\n",
        "num_filter = 35\n",
        "compression = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv9R1EoDJMJs",
        "outputId": "d5a00f86-6ce7-4fae-90e1-740473f955fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d47TRlrj5hTO"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvQZ3OPU53mL"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMzhtYvl5mVy"
      },
      "source": [
        "traindataGen=ImageDataGenerator(shear_range=0.2,zoom_range=0.2, rotation_range=20, width_shift_range=0.2,\n",
        "height_shift_range=0.2,horizontal_flip=True,fill_mode='nearest')\n",
        "testdataGen=ImageDataGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_yH650l6N4N"
      },
      "source": [
        "traindataGen.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbffjUklJfct",
        "outputId": "6ca0ece9-2782-4314-feb6-6d40a3800af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('X_train shape =', X_train.shape)\n",
        "print('X_test shape =', X_test.shape)\n",
        "print('y_train shape =', y_train.shape)\n",
        "print('y_test shape =', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape = (50000, 32, 32, 3)\n",
            "X_test shape = (10000, 32, 32, 3)\n",
            "y_train shape = (50000, 10)\n",
            "y_test shape = (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTR1DJW5XyQZ"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5smh6wDYwAT"
      },
      "source": [
        "import os\n",
        "os.mkdir('model_save')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrXDlmTNVx4W"
      },
      "source": [
        "import tensorflow.keras.backend as k\n",
        "def changeLearningRate(epoch):\n",
        "  ''' This function changes learing rate based on validation accuracy and epoch number'''\n",
        "  learning_rate = k.eval(model.optimizer.lr)\n",
        "  if epoch == epochs/2:\n",
        "     #The learning rate is divided by 10 at 50% of the total number of training epochs\n",
        "    learning_rate = learning_rate * 0.1\n",
        "   #The learning rate is divided by 10 at 75% of the total number of training epochs\n",
        "  elif epoch == (epochs*3)/4:\n",
        "      learning_rate = learning_rate * 0.1\n",
        "  return learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulLZ6t4_Jlbh"
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JOhzAGbJpPG"
      },
      "source": [
        "l = 6\n",
        "dropout_rate = 0\n",
        "input = layers.Input(shape=(img_height, img_width, channel))\n",
        "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate=0)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate=0)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate=0)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate=0)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate=0)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate=0)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate=0)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAeExyIlKYUv",
        "outputId": "32157bef-0326-4e05-8ecb-94653e81c931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=input, outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 35)   945         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 35)   140         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 35)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 35)   11025       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 70)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 70)   280         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 70)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 35)   22050       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 105)  0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 105)  420         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 105)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 35)   33075       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 140)  0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 140)  560         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 140)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 35)   44100       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 175)  0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 175)  700         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 175)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 35)   55125       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 210)  0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 210)  840         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 210)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 35)   66150       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 245)  0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 245)  980         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 245)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 35)   8575        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 35)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 35)   140         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 35)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 35)   11025       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 70)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 70)   280         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 70)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 35)   22050       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 16, 16, 105)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 105)  420         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 105)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 35)   33075       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 16, 16, 140)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 140)  560         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 140)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 35)   44100       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 16, 16, 175)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 175)  700         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 175)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 35)   55125       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 16, 16, 210)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 210)  840         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 210)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 35)   66150       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 16, 16, 245)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 245)  980         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 245)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 35)   8575        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 35)     0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 35)     140         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 35)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 35)     11025       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 8, 8, 70)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 70)     280         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 70)     0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 35)     22050       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 8, 8, 105)    0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 105)    420         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 105)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 35)     33075       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 8, 8, 140)    0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 140)    560         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 8, 140)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 35)     44100       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 8, 8, 175)    0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 175)    700         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 8, 175)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 35)     55125       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 8, 8, 210)    0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 210)    840         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 210)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 35)     66150       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 8, 8, 245)    0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 245)    980         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 245)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 35)     8575        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 35)     0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 4, 4, 35)     140         average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 4, 4, 35)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 4, 4, 35)     11025       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 4, 4, 70)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 4, 4, 70)     280         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 4, 4, 70)     0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 4, 4, 35)     22050       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 4, 4, 105)    0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 4, 4, 105)    420         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 4, 4, 105)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 4, 4, 35)     33075       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 4, 4, 140)    0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 4, 4, 140)    560         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 4, 4, 140)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 4, 4, 35)     44100       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 4, 4, 175)    0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 4, 4, 175)    700         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 4, 4, 175)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 4, 4, 35)     55125       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 4, 4, 210)    0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 4, 4, 210)    840         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 4, 4, 210)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 4, 4, 35)     66150       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 4, 4, 245)    0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 4, 4, 245)    980         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 4, 4, 245)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 245)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 980)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           9810        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 978,260\n",
            "Trainable params: 970,420\n",
            "Non-trainable params: 7,840\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kWhC0Cj6zYq"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
        "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd7_cxUwYjkY"
      },
      "source": [
        "#lrschedule = LearningRateScheduler(changeLearningRate, verbose=1)\n",
        "filepath = 'model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
        "callback_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiJGybtJ6xSp",
        "outputId": "7bc91ed6-ff8c-4dd0-e134-d0ff24ebf681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(traindataGen.flow(X_train, y_train, batch_size=128), steps_per_epoch=len(X_train) / 128, epochs=epochs, validation_data=testdataGen.flow(X_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 2.1071 - accuracy: 0.3547\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.32390, saving model to model_save/weights-01-0.3239.hdf5\n",
            "391/390 [==============================] - 38s 98ms/step - loss: 2.1071 - accuracy: 0.3547 - val_loss: 2.1335 - val_accuracy: 0.3239\n",
            "Epoch 2/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 2.0067 - accuracy: 0.4546\n",
            "Epoch 00002: val_accuracy improved from 0.32390 to 0.36550, saving model to model_save/weights-02-0.3655.hdf5\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 2.0067 - accuracy: 0.4546 - val_loss: 2.0883 - val_accuracy: 0.3655\n",
            "Epoch 3/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.9625 - accuracy: 0.4962\n",
            "Epoch 00003: val_accuracy did not improve from 0.36550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.9625 - accuracy: 0.4962 - val_loss: 2.1790 - val_accuracy: 0.2747\n",
            "Epoch 4/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.9286 - accuracy: 0.5320\n",
            "Epoch 00004: val_accuracy improved from 0.36550 to 0.46080, saving model to model_save/weights-04-0.4608.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.9286 - accuracy: 0.5320 - val_loss: 1.9942 - val_accuracy: 0.4608\n",
            "Epoch 5/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.9058 - accuracy: 0.5551\n",
            "Epoch 00005: val_accuracy improved from 0.46080 to 0.47440, saving model to model_save/weights-05-0.4744.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.9058 - accuracy: 0.5551 - val_loss: 1.9820 - val_accuracy: 0.4744\n",
            "Epoch 6/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.8870 - accuracy: 0.5737\n",
            "Epoch 00006: val_accuracy did not improve from 0.47440\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.8870 - accuracy: 0.5737 - val_loss: 2.0517 - val_accuracy: 0.4022\n",
            "Epoch 7/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.8654 - accuracy: 0.5954\n",
            "Epoch 00007: val_accuracy improved from 0.47440 to 0.58140, saving model to model_save/weights-07-0.5814.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.8654 - accuracy: 0.5954 - val_loss: 1.8788 - val_accuracy: 0.5814\n",
            "Epoch 8/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.8507 - accuracy: 0.6098\n",
            "Epoch 00008: val_accuracy improved from 0.58140 to 0.59400, saving model to model_save/weights-08-0.5940.hdf5\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.8507 - accuracy: 0.6098 - val_loss: 1.8655 - val_accuracy: 0.5940\n",
            "Epoch 9/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.8256 - accuracy: 0.6363\n",
            "Epoch 00009: val_accuracy did not improve from 0.59400\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.8256 - accuracy: 0.6363 - val_loss: 2.0746 - val_accuracy: 0.3784\n",
            "Epoch 10/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.8061 - accuracy: 0.6556\n",
            "Epoch 00010: val_accuracy improved from 0.59400 to 0.64440, saving model to model_save/weights-10-0.6444.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.8061 - accuracy: 0.6556 - val_loss: 1.8177 - val_accuracy: 0.6444\n",
            "Epoch 11/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7948 - accuracy: 0.6668\n",
            "Epoch 00011: val_accuracy did not improve from 0.64440\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.7948 - accuracy: 0.6668 - val_loss: 1.8251 - val_accuracy: 0.6346\n",
            "Epoch 12/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7783 - accuracy: 0.6832\n",
            "Epoch 00012: val_accuracy improved from 0.64440 to 0.68200, saving model to model_save/weights-12-0.6820.hdf5\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.7783 - accuracy: 0.6832 - val_loss: 1.7775 - val_accuracy: 0.6820\n",
            "Epoch 13/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7729 - accuracy: 0.6883\n",
            "Epoch 00013: val_accuracy did not improve from 0.68200\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7729 - accuracy: 0.6883 - val_loss: 1.8025 - val_accuracy: 0.6546\n",
            "Epoch 14/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7596 - accuracy: 0.7012\n",
            "Epoch 00014: val_accuracy did not improve from 0.68200\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.7596 - accuracy: 0.7012 - val_loss: 1.7860 - val_accuracy: 0.6732\n",
            "Epoch 15/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7546 - accuracy: 0.7065\n",
            "Epoch 00015: val_accuracy did not improve from 0.68200\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7546 - accuracy: 0.7065 - val_loss: 1.7977 - val_accuracy: 0.6603\n",
            "Epoch 16/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7450 - accuracy: 0.7160\n",
            "Epoch 00016: val_accuracy did not improve from 0.68200\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7450 - accuracy: 0.7160 - val_loss: 1.7950 - val_accuracy: 0.6651\n",
            "Epoch 17/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7368 - accuracy: 0.7249\n",
            "Epoch 00017: val_accuracy improved from 0.68200 to 0.70570, saving model to model_save/weights-17-0.7057.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.7368 - accuracy: 0.7249 - val_loss: 1.7532 - val_accuracy: 0.7057\n",
            "Epoch 18/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7321 - accuracy: 0.7291\n",
            "Epoch 00018: val_accuracy did not improve from 0.70570\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7321 - accuracy: 0.7291 - val_loss: 1.7752 - val_accuracy: 0.6856\n",
            "Epoch 19/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7289 - accuracy: 0.7321\n",
            "Epoch 00019: val_accuracy did not improve from 0.70570\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7289 - accuracy: 0.7321 - val_loss: 1.9077 - val_accuracy: 0.5495\n",
            "Epoch 20/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7237 - accuracy: 0.7371\n",
            "Epoch 00020: val_accuracy improved from 0.70570 to 0.74830, saving model to model_save/weights-20-0.7483.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7237 - accuracy: 0.7371 - val_loss: 1.7133 - val_accuracy: 0.7483\n",
            "Epoch 21/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7170 - accuracy: 0.7449\n",
            "Epoch 00021: val_accuracy did not improve from 0.74830\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7170 - accuracy: 0.7449 - val_loss: 1.8125 - val_accuracy: 0.6460\n",
            "Epoch 22/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7141 - accuracy: 0.7467\n",
            "Epoch 00022: val_accuracy did not improve from 0.74830\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.7141 - accuracy: 0.7467 - val_loss: 1.7599 - val_accuracy: 0.6992\n",
            "Epoch 23/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7070 - accuracy: 0.7545\n",
            "Epoch 00023: val_accuracy did not improve from 0.74830\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7070 - accuracy: 0.7545 - val_loss: 1.8403 - val_accuracy: 0.6178\n",
            "Epoch 24/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7025 - accuracy: 0.7587\n",
            "Epoch 00024: val_accuracy did not improve from 0.74830\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7025 - accuracy: 0.7587 - val_loss: 1.9059 - val_accuracy: 0.5504\n",
            "Epoch 25/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.7000 - accuracy: 0.7615\n",
            "Epoch 00025: val_accuracy improved from 0.74830 to 0.75660, saving model to model_save/weights-25-0.7566.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.7000 - accuracy: 0.7615 - val_loss: 1.7038 - val_accuracy: 0.7566\n",
            "Epoch 26/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6994 - accuracy: 0.7614\n",
            "Epoch 00026: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6994 - accuracy: 0.7614 - val_loss: 1.7406 - val_accuracy: 0.7201\n",
            "Epoch 27/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6935 - accuracy: 0.7679\n",
            "Epoch 00027: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6935 - accuracy: 0.7679 - val_loss: 1.8256 - val_accuracy: 0.6336\n",
            "Epoch 28/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6906 - accuracy: 0.7713\n",
            "Epoch 00028: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.6906 - accuracy: 0.7713 - val_loss: 1.7598 - val_accuracy: 0.7001\n",
            "Epoch 29/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6889 - accuracy: 0.7721\n",
            "Epoch 00029: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6889 - accuracy: 0.7721 - val_loss: 1.8055 - val_accuracy: 0.6523\n",
            "Epoch 30/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6841 - accuracy: 0.7774\n",
            "Epoch 00030: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6841 - accuracy: 0.7774 - val_loss: 1.7560 - val_accuracy: 0.7033\n",
            "Epoch 31/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6838 - accuracy: 0.7770\n",
            "Epoch 00031: val_accuracy did not improve from 0.75660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6838 - accuracy: 0.7770 - val_loss: 1.7083 - val_accuracy: 0.7505\n",
            "Epoch 32/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6801 - accuracy: 0.7803\n",
            "Epoch 00032: val_accuracy improved from 0.75660 to 0.76000, saving model to model_save/weights-32-0.7600.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6801 - accuracy: 0.7803 - val_loss: 1.6996 - val_accuracy: 0.7600\n",
            "Epoch 33/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6753 - accuracy: 0.7859\n",
            "Epoch 00033: val_accuracy did not improve from 0.76000\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6753 - accuracy: 0.7859 - val_loss: 1.7211 - val_accuracy: 0.7383\n",
            "Epoch 34/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6716 - accuracy: 0.7892\n",
            "Epoch 00034: val_accuracy improved from 0.76000 to 0.77680, saving model to model_save/weights-34-0.7768.hdf5\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.6716 - accuracy: 0.7892 - val_loss: 1.6824 - val_accuracy: 0.7768\n",
            "Epoch 35/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6731 - accuracy: 0.7888\n",
            "Epoch 00035: val_accuracy improved from 0.77680 to 0.78740, saving model to model_save/weights-35-0.7874.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6731 - accuracy: 0.7888 - val_loss: 1.6717 - val_accuracy: 0.7874\n",
            "Epoch 36/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6684 - accuracy: 0.7929\n",
            "Epoch 00036: val_accuracy improved from 0.78740 to 0.80420, saving model to model_save/weights-36-0.8042.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6684 - accuracy: 0.7929 - val_loss: 1.6584 - val_accuracy: 0.8042\n",
            "Epoch 37/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6665 - accuracy: 0.7947\n",
            "Epoch 00037: val_accuracy did not improve from 0.80420\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6665 - accuracy: 0.7947 - val_loss: 1.6753 - val_accuracy: 0.7859\n",
            "Epoch 38/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6620 - accuracy: 0.7994\n",
            "Epoch 00038: val_accuracy did not improve from 0.80420\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.6620 - accuracy: 0.7994 - val_loss: 1.7392 - val_accuracy: 0.7205\n",
            "Epoch 39/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6626 - accuracy: 0.7982\n",
            "Epoch 00039: val_accuracy did not improve from 0.80420\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6626 - accuracy: 0.7982 - val_loss: 1.8276 - val_accuracy: 0.6306\n",
            "Epoch 40/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6606 - accuracy: 0.8002\n",
            "Epoch 00040: val_accuracy did not improve from 0.80420\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6606 - accuracy: 0.8002 - val_loss: 1.7253 - val_accuracy: 0.7345\n",
            "Epoch 41/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6558 - accuracy: 0.8063\n",
            "Epoch 00041: val_accuracy did not improve from 0.80420\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6558 - accuracy: 0.8063 - val_loss: 1.7385 - val_accuracy: 0.7208\n",
            "Epoch 42/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6572 - accuracy: 0.8035\n",
            "Epoch 00042: val_accuracy improved from 0.80420 to 0.80430, saving model to model_save/weights-42-0.8043.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6572 - accuracy: 0.8035 - val_loss: 1.6554 - val_accuracy: 0.8043\n",
            "Epoch 43/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6516 - accuracy: 0.8088\n",
            "Epoch 00043: val_accuracy did not improve from 0.80430\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6516 - accuracy: 0.8088 - val_loss: 1.6965 - val_accuracy: 0.7644\n",
            "Epoch 44/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6508 - accuracy: 0.8105\n",
            "Epoch 00044: val_accuracy did not improve from 0.80430\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6508 - accuracy: 0.8105 - val_loss: 1.7200 - val_accuracy: 0.7381\n",
            "Epoch 45/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6513 - accuracy: 0.8097\n",
            "Epoch 00045: val_accuracy did not improve from 0.80430\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6513 - accuracy: 0.8097 - val_loss: 1.7032 - val_accuracy: 0.7571\n",
            "Epoch 46/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6498 - accuracy: 0.8118\n",
            "Epoch 00046: val_accuracy did not improve from 0.80430\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6498 - accuracy: 0.8118 - val_loss: 1.7315 - val_accuracy: 0.7293\n",
            "Epoch 47/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6472 - accuracy: 0.8137\n",
            "Epoch 00047: val_accuracy did not improve from 0.80430\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6472 - accuracy: 0.8137 - val_loss: 1.7099 - val_accuracy: 0.7502\n",
            "Epoch 48/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6440 - accuracy: 0.8173\n",
            "Epoch 00048: val_accuracy improved from 0.80430 to 0.80780, saving model to model_save/weights-48-0.8078.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6440 - accuracy: 0.8173 - val_loss: 1.6519 - val_accuracy: 0.8078\n",
            "Epoch 49/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6423 - accuracy: 0.8190\n",
            "Epoch 00049: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6423 - accuracy: 0.8190 - val_loss: 1.6590 - val_accuracy: 0.8012\n",
            "Epoch 50/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6441 - accuracy: 0.8170\n",
            "Epoch 00050: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6441 - accuracy: 0.8170 - val_loss: 1.7160 - val_accuracy: 0.7426\n",
            "Epoch 51/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6384 - accuracy: 0.8230\n",
            "Epoch 00051: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6384 - accuracy: 0.8230 - val_loss: 1.6602 - val_accuracy: 0.7995\n",
            "Epoch 52/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6379 - accuracy: 0.8241\n",
            "Epoch 00052: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6379 - accuracy: 0.8241 - val_loss: 1.6809 - val_accuracy: 0.7784\n",
            "Epoch 53/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6373 - accuracy: 0.8239\n",
            "Epoch 00053: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6373 - accuracy: 0.8239 - val_loss: 1.6731 - val_accuracy: 0.7860\n",
            "Epoch 54/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6357 - accuracy: 0.8258\n",
            "Epoch 00054: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6357 - accuracy: 0.8258 - val_loss: 1.6727 - val_accuracy: 0.7870\n",
            "Epoch 55/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6337 - accuracy: 0.8273\n",
            "Epoch 00055: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6337 - accuracy: 0.8273 - val_loss: 1.6616 - val_accuracy: 0.7978\n",
            "Epoch 56/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6305 - accuracy: 0.8311\n",
            "Epoch 00056: val_accuracy did not improve from 0.80780\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6305 - accuracy: 0.8311 - val_loss: 1.6833 - val_accuracy: 0.7761\n",
            "Epoch 57/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6323 - accuracy: 0.8295\n",
            "Epoch 00057: val_accuracy improved from 0.80780 to 0.80860, saving model to model_save/weights-57-0.8086.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6323 - accuracy: 0.8295 - val_loss: 1.6519 - val_accuracy: 0.8086\n",
            "Epoch 58/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6313 - accuracy: 0.8301\n",
            "Epoch 00058: val_accuracy did not improve from 0.80860\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.6313 - accuracy: 0.8301 - val_loss: 1.6632 - val_accuracy: 0.7969\n",
            "Epoch 59/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6284 - accuracy: 0.8331\n",
            "Epoch 00059: val_accuracy did not improve from 0.80860\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.6284 - accuracy: 0.8331 - val_loss: 1.6849 - val_accuracy: 0.7761\n",
            "Epoch 60/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6257 - accuracy: 0.8353\n",
            "Epoch 00060: val_accuracy did not improve from 0.80860\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6257 - accuracy: 0.8353 - val_loss: 1.6662 - val_accuracy: 0.7939\n",
            "Epoch 61/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6254 - accuracy: 0.8360\n",
            "Epoch 00061: val_accuracy improved from 0.80860 to 0.81100, saving model to model_save/weights-61-0.8110.hdf5\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.6254 - accuracy: 0.8360 - val_loss: 1.6492 - val_accuracy: 0.8110\n",
            "Epoch 62/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6231 - accuracy: 0.8385\n",
            "Epoch 00062: val_accuracy did not improve from 0.81100\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.6231 - accuracy: 0.8385 - val_loss: 1.6812 - val_accuracy: 0.7798\n",
            "Epoch 63/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6255 - accuracy: 0.8362\n",
            "Epoch 00063: val_accuracy improved from 0.81100 to 0.81460, saving model to model_save/weights-63-0.8146.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6255 - accuracy: 0.8362 - val_loss: 1.6445 - val_accuracy: 0.8146\n",
            "Epoch 64/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6226 - accuracy: 0.8383\n",
            "Epoch 00064: val_accuracy did not improve from 0.81460\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6226 - accuracy: 0.8383 - val_loss: 1.6783 - val_accuracy: 0.7815\n",
            "Epoch 65/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6202 - accuracy: 0.8411\n",
            "Epoch 00065: val_accuracy improved from 0.81460 to 0.83660, saving model to model_save/weights-65-0.8366.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6202 - accuracy: 0.8411 - val_loss: 1.6235 - val_accuracy: 0.8366\n",
            "Epoch 66/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6206 - accuracy: 0.8407\n",
            "Epoch 00066: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6206 - accuracy: 0.8407 - val_loss: 1.6595 - val_accuracy: 0.8006\n",
            "Epoch 67/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6201 - accuracy: 0.8413\n",
            "Epoch 00067: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6201 - accuracy: 0.8413 - val_loss: 1.6312 - val_accuracy: 0.8300\n",
            "Epoch 68/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6193 - accuracy: 0.8424\n",
            "Epoch 00068: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6193 - accuracy: 0.8424 - val_loss: 1.6641 - val_accuracy: 0.7957\n",
            "Epoch 69/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6191 - accuracy: 0.8415\n",
            "Epoch 00069: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6191 - accuracy: 0.8415 - val_loss: 1.6538 - val_accuracy: 0.8069\n",
            "Epoch 70/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6166 - accuracy: 0.8442\n",
            "Epoch 00070: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6166 - accuracy: 0.8442 - val_loss: 1.6351 - val_accuracy: 0.8251\n",
            "Epoch 71/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6132 - accuracy: 0.8478\n",
            "Epoch 00071: val_accuracy did not improve from 0.83660\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6132 - accuracy: 0.8478 - val_loss: 1.6435 - val_accuracy: 0.8172\n",
            "Epoch 72/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6139 - accuracy: 0.8475\n",
            "Epoch 00072: val_accuracy improved from 0.83660 to 0.84080, saving model to model_save/weights-72-0.8408.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6139 - accuracy: 0.8475 - val_loss: 1.6200 - val_accuracy: 0.8408\n",
            "Epoch 73/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6129 - accuracy: 0.8479\n",
            "Epoch 00073: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6129 - accuracy: 0.8479 - val_loss: 1.6464 - val_accuracy: 0.8133\n",
            "Epoch 74/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6129 - accuracy: 0.8488\n",
            "Epoch 00074: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6129 - accuracy: 0.8488 - val_loss: 1.6677 - val_accuracy: 0.7925\n",
            "Epoch 75/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6121 - accuracy: 0.8491\n",
            "Epoch 00075: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6121 - accuracy: 0.8491 - val_loss: 1.6302 - val_accuracy: 0.8305\n",
            "Epoch 76/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6101 - accuracy: 0.8514\n",
            "Epoch 00076: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6101 - accuracy: 0.8514 - val_loss: 1.6804 - val_accuracy: 0.7802\n",
            "Epoch 77/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6112 - accuracy: 0.8494\n",
            "Epoch 00077: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6112 - accuracy: 0.8494 - val_loss: 1.6485 - val_accuracy: 0.8116\n",
            "Epoch 78/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6104 - accuracy: 0.8505\n",
            "Epoch 00078: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6104 - accuracy: 0.8505 - val_loss: 1.6614 - val_accuracy: 0.7989\n",
            "Epoch 79/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6087 - accuracy: 0.8531\n",
            "Epoch 00079: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6087 - accuracy: 0.8531 - val_loss: 1.6442 - val_accuracy: 0.8151\n",
            "Epoch 80/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6079 - accuracy: 0.8534\n",
            "Epoch 00080: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6079 - accuracy: 0.8534 - val_loss: 1.6346 - val_accuracy: 0.8249\n",
            "Epoch 81/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6077 - accuracy: 0.8539\n",
            "Epoch 00081: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6077 - accuracy: 0.8539 - val_loss: 1.6355 - val_accuracy: 0.8251\n",
            "Epoch 82/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6075 - accuracy: 0.8539\n",
            "Epoch 00082: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6075 - accuracy: 0.8539 - val_loss: 1.6256 - val_accuracy: 0.8339\n",
            "Epoch 83/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6043 - accuracy: 0.8569\n",
            "Epoch 00083: val_accuracy did not improve from 0.84080\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.6043 - accuracy: 0.8569 - val_loss: 1.6280 - val_accuracy: 0.8326\n",
            "Epoch 84/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6042 - accuracy: 0.8573\n",
            "Epoch 00084: val_accuracy improved from 0.84080 to 0.84940, saving model to model_save/weights-84-0.8494.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6042 - accuracy: 0.8573 - val_loss: 1.6108 - val_accuracy: 0.8494\n",
            "Epoch 85/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6035 - accuracy: 0.8580\n",
            "Epoch 00085: val_accuracy did not improve from 0.84940\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6035 - accuracy: 0.8580 - val_loss: 1.6487 - val_accuracy: 0.8111\n",
            "Epoch 86/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6038 - accuracy: 0.8577\n",
            "Epoch 00086: val_accuracy improved from 0.84940 to 0.85630, saving model to model_save/weights-86-0.8563.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.6038 - accuracy: 0.8577 - val_loss: 1.6042 - val_accuracy: 0.8563\n",
            "Epoch 87/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6013 - accuracy: 0.8606\n",
            "Epoch 00087: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.6013 - accuracy: 0.8606 - val_loss: 1.6497 - val_accuracy: 0.8109\n",
            "Epoch 88/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6010 - accuracy: 0.8604\n",
            "Epoch 00088: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.6010 - accuracy: 0.8604 - val_loss: 1.6071 - val_accuracy: 0.8534\n",
            "Epoch 89/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.6016 - accuracy: 0.8603\n",
            "Epoch 00089: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.6016 - accuracy: 0.8603 - val_loss: 1.6212 - val_accuracy: 0.8399\n",
            "Epoch 90/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5991 - accuracy: 0.8621\n",
            "Epoch 00090: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5991 - accuracy: 0.8621 - val_loss: 1.6334 - val_accuracy: 0.8276\n",
            "Epoch 91/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5995 - accuracy: 0.8604\n",
            "Epoch 00091: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5995 - accuracy: 0.8604 - val_loss: 1.6523 - val_accuracy: 0.8087\n",
            "Epoch 92/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5980 - accuracy: 0.8635\n",
            "Epoch 00092: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5980 - accuracy: 0.8635 - val_loss: 1.6282 - val_accuracy: 0.8329\n",
            "Epoch 93/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5981 - accuracy: 0.8633\n",
            "Epoch 00093: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5981 - accuracy: 0.8633 - val_loss: 1.6595 - val_accuracy: 0.8004\n",
            "Epoch 94/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5959 - accuracy: 0.8654\n",
            "Epoch 00094: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5959 - accuracy: 0.8654 - val_loss: 1.6146 - val_accuracy: 0.8455\n",
            "Epoch 95/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5943 - accuracy: 0.8674\n",
            "Epoch 00095: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5943 - accuracy: 0.8674 - val_loss: 1.6318 - val_accuracy: 0.8293\n",
            "Epoch 96/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5972 - accuracy: 0.8641\n",
            "Epoch 00096: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5972 - accuracy: 0.8641 - val_loss: 1.6347 - val_accuracy: 0.8254\n",
            "Epoch 97/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5967 - accuracy: 0.8652\n",
            "Epoch 00097: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5967 - accuracy: 0.8652 - val_loss: 1.6159 - val_accuracy: 0.8441\n",
            "Epoch 98/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5965 - accuracy: 0.8651\n",
            "Epoch 00098: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5965 - accuracy: 0.8651 - val_loss: 1.6466 - val_accuracy: 0.8132\n",
            "Epoch 99/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5940 - accuracy: 0.8671\n",
            "Epoch 00099: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5940 - accuracy: 0.8671 - val_loss: 1.6340 - val_accuracy: 0.8253\n",
            "Epoch 100/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5938 - accuracy: 0.8675\n",
            "Epoch 00100: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5938 - accuracy: 0.8675 - val_loss: 1.6651 - val_accuracy: 0.7949\n",
            "Epoch 101/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5951 - accuracy: 0.8659\n",
            "Epoch 00101: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5951 - accuracy: 0.8659 - val_loss: 1.6524 - val_accuracy: 0.8075\n",
            "Epoch 102/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5918 - accuracy: 0.8698\n",
            "Epoch 00102: val_accuracy did not improve from 0.85630\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5918 - accuracy: 0.8698 - val_loss: 1.6414 - val_accuracy: 0.8193\n",
            "Epoch 103/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5905 - accuracy: 0.8709\n",
            "Epoch 00103: val_accuracy improved from 0.85630 to 0.86150, saving model to model_save/weights-103-0.8615.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5905 - accuracy: 0.8709 - val_loss: 1.5991 - val_accuracy: 0.8615\n",
            "Epoch 104/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5912 - accuracy: 0.8702\n",
            "Epoch 00104: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5912 - accuracy: 0.8702 - val_loss: 1.6490 - val_accuracy: 0.8111\n",
            "Epoch 105/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5890 - accuracy: 0.8726\n",
            "Epoch 00105: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5890 - accuracy: 0.8726 - val_loss: 1.6122 - val_accuracy: 0.8499\n",
            "Epoch 106/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5908 - accuracy: 0.8708\n",
            "Epoch 00106: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5908 - accuracy: 0.8708 - val_loss: 1.6207 - val_accuracy: 0.8404\n",
            "Epoch 107/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5911 - accuracy: 0.8701\n",
            "Epoch 00107: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5911 - accuracy: 0.8701 - val_loss: 1.6575 - val_accuracy: 0.8032\n",
            "Epoch 108/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5876 - accuracy: 0.8735\n",
            "Epoch 00108: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5876 - accuracy: 0.8735 - val_loss: 1.6309 - val_accuracy: 0.8296\n",
            "Epoch 109/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5880 - accuracy: 0.8738\n",
            "Epoch 00109: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5880 - accuracy: 0.8738 - val_loss: 1.6191 - val_accuracy: 0.8408\n",
            "Epoch 110/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5892 - accuracy: 0.8721\n",
            "Epoch 00110: val_accuracy did not improve from 0.86150\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5892 - accuracy: 0.8721 - val_loss: 1.6505 - val_accuracy: 0.8092\n",
            "Epoch 111/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5862 - accuracy: 0.8755\n",
            "Epoch 00111: val_accuracy improved from 0.86150 to 0.86840, saving model to model_save/weights-111-0.8684.hdf5\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5862 - accuracy: 0.8755 - val_loss: 1.5917 - val_accuracy: 0.8684\n",
            "Epoch 112/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5858 - accuracy: 0.8759\n",
            "Epoch 00112: val_accuracy improved from 0.86840 to 0.87010, saving model to model_save/weights-112-0.8701.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5858 - accuracy: 0.8759 - val_loss: 1.5914 - val_accuracy: 0.8701\n",
            "Epoch 113/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5826 - accuracy: 0.8787\n",
            "Epoch 00113: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5826 - accuracy: 0.8787 - val_loss: 1.6009 - val_accuracy: 0.8612\n",
            "Epoch 114/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5837 - accuracy: 0.8780\n",
            "Epoch 00114: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5837 - accuracy: 0.8780 - val_loss: 1.6083 - val_accuracy: 0.8527\n",
            "Epoch 115/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5839 - accuracy: 0.8772\n",
            "Epoch 00115: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5839 - accuracy: 0.8772 - val_loss: 1.5946 - val_accuracy: 0.8656\n",
            "Epoch 116/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5841 - accuracy: 0.8768\n",
            "Epoch 00116: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5841 - accuracy: 0.8768 - val_loss: 1.6208 - val_accuracy: 0.8390\n",
            "Epoch 117/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5829 - accuracy: 0.8786\n",
            "Epoch 00117: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5829 - accuracy: 0.8786 - val_loss: 1.6004 - val_accuracy: 0.8607\n",
            "Epoch 118/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5819 - accuracy: 0.8794\n",
            "Epoch 00118: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5819 - accuracy: 0.8794 - val_loss: 1.6386 - val_accuracy: 0.8217\n",
            "Epoch 119/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5814 - accuracy: 0.8798\n",
            "Epoch 00119: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5814 - accuracy: 0.8798 - val_loss: 1.5972 - val_accuracy: 0.8633\n",
            "Epoch 120/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5811 - accuracy: 0.8798\n",
            "Epoch 00120: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5811 - accuracy: 0.8798 - val_loss: 1.6165 - val_accuracy: 0.8453\n",
            "Epoch 121/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5798 - accuracy: 0.8814\n",
            "Epoch 00121: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5798 - accuracy: 0.8814 - val_loss: 1.6201 - val_accuracy: 0.8396\n",
            "Epoch 122/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5803 - accuracy: 0.8811\n",
            "Epoch 00122: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5803 - accuracy: 0.8811 - val_loss: 1.6021 - val_accuracy: 0.8580\n",
            "Epoch 123/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5806 - accuracy: 0.8806\n",
            "Epoch 00123: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5806 - accuracy: 0.8806 - val_loss: 1.6451 - val_accuracy: 0.8146\n",
            "Epoch 124/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5796 - accuracy: 0.8818\n",
            "Epoch 00124: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5796 - accuracy: 0.8818 - val_loss: 1.6136 - val_accuracy: 0.8469\n",
            "Epoch 125/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5792 - accuracy: 0.8826\n",
            "Epoch 00125: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5792 - accuracy: 0.8826 - val_loss: 1.6445 - val_accuracy: 0.8148\n",
            "Epoch 126/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5803 - accuracy: 0.8810\n",
            "Epoch 00126: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5803 - accuracy: 0.8810 - val_loss: 1.6081 - val_accuracy: 0.8528\n",
            "Epoch 127/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5784 - accuracy: 0.8829\n",
            "Epoch 00127: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5784 - accuracy: 0.8829 - val_loss: 1.6143 - val_accuracy: 0.8467\n",
            "Epoch 128/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5765 - accuracy: 0.8845\n",
            "Epoch 00128: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5765 - accuracy: 0.8845 - val_loss: 1.6085 - val_accuracy: 0.8512\n",
            "Epoch 129/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5771 - accuracy: 0.8843\n",
            "Epoch 00129: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5771 - accuracy: 0.8843 - val_loss: 1.5999 - val_accuracy: 0.8602\n",
            "Epoch 130/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5775 - accuracy: 0.8840\n",
            "Epoch 00130: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5775 - accuracy: 0.8840 - val_loss: 1.6230 - val_accuracy: 0.8379\n",
            "Epoch 131/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5776 - accuracy: 0.8838\n",
            "Epoch 00131: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5776 - accuracy: 0.8838 - val_loss: 1.6000 - val_accuracy: 0.8600\n",
            "Epoch 132/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5758 - accuracy: 0.8858\n",
            "Epoch 00132: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5758 - accuracy: 0.8858 - val_loss: 1.5991 - val_accuracy: 0.8623\n",
            "Epoch 133/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5765 - accuracy: 0.8849\n",
            "Epoch 00133: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5765 - accuracy: 0.8849 - val_loss: 1.6021 - val_accuracy: 0.8581\n",
            "Epoch 134/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5727 - accuracy: 0.8888\n",
            "Epoch 00134: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5727 - accuracy: 0.8888 - val_loss: 1.6162 - val_accuracy: 0.8441\n",
            "Epoch 135/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5736 - accuracy: 0.8878\n",
            "Epoch 00135: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5736 - accuracy: 0.8878 - val_loss: 1.5983 - val_accuracy: 0.8627\n",
            "Epoch 136/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5754 - accuracy: 0.8862\n",
            "Epoch 00136: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5754 - accuracy: 0.8862 - val_loss: 1.5964 - val_accuracy: 0.8636\n",
            "Epoch 137/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5744 - accuracy: 0.8868\n",
            "Epoch 00137: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5744 - accuracy: 0.8868 - val_loss: 1.6076 - val_accuracy: 0.8526\n",
            "Epoch 138/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5722 - accuracy: 0.8892\n",
            "Epoch 00138: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5722 - accuracy: 0.8892 - val_loss: 1.6369 - val_accuracy: 0.8237\n",
            "Epoch 139/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5741 - accuracy: 0.8874\n",
            "Epoch 00139: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5741 - accuracy: 0.8874 - val_loss: 1.6088 - val_accuracy: 0.8511\n",
            "Epoch 140/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5733 - accuracy: 0.8882\n",
            "Epoch 00140: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5733 - accuracy: 0.8882 - val_loss: 1.6279 - val_accuracy: 0.8322\n",
            "Epoch 141/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5731 - accuracy: 0.8884\n",
            "Epoch 00141: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5731 - accuracy: 0.8884 - val_loss: 1.6015 - val_accuracy: 0.8590\n",
            "Epoch 142/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5729 - accuracy: 0.8886\n",
            "Epoch 00142: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5729 - accuracy: 0.8886 - val_loss: 1.6313 - val_accuracy: 0.8296\n",
            "Epoch 143/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5702 - accuracy: 0.8913\n",
            "Epoch 00143: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5702 - accuracy: 0.8913 - val_loss: 1.6025 - val_accuracy: 0.8566\n",
            "Epoch 144/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5699 - accuracy: 0.8915\n",
            "Epoch 00144: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5699 - accuracy: 0.8915 - val_loss: 1.5918 - val_accuracy: 0.8689\n",
            "Epoch 145/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5697 - accuracy: 0.8915\n",
            "Epoch 00145: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5697 - accuracy: 0.8915 - val_loss: 1.5978 - val_accuracy: 0.8626\n",
            "Epoch 146/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5703 - accuracy: 0.8909\n",
            "Epoch 00146: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5703 - accuracy: 0.8909 - val_loss: 1.6047 - val_accuracy: 0.8560\n",
            "Epoch 147/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5701 - accuracy: 0.8911\n",
            "Epoch 00147: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5701 - accuracy: 0.8911 - val_loss: 1.6045 - val_accuracy: 0.8576\n",
            "Epoch 148/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5706 - accuracy: 0.8909\n",
            "Epoch 00148: val_accuracy did not improve from 0.87010\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5706 - accuracy: 0.8909 - val_loss: 1.5999 - val_accuracy: 0.8615\n",
            "Epoch 149/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5689 - accuracy: 0.8926\n",
            "Epoch 00149: val_accuracy improved from 0.87010 to 0.87160, saving model to model_save/weights-149-0.8716.hdf5\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5689 - accuracy: 0.8926 - val_loss: 1.5881 - val_accuracy: 0.8716\n",
            "Epoch 150/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5702 - accuracy: 0.8907\n",
            "Epoch 00150: val_accuracy improved from 0.87160 to 0.87940, saving model to model_save/weights-150-0.8794.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5702 - accuracy: 0.8907 - val_loss: 1.5817 - val_accuracy: 0.8794\n",
            "Epoch 151/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5700 - accuracy: 0.8911\n",
            "Epoch 00151: val_accuracy did not improve from 0.87940\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5700 - accuracy: 0.8911 - val_loss: 1.5907 - val_accuracy: 0.8695\n",
            "Epoch 152/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5674 - accuracy: 0.8933\n",
            "Epoch 00152: val_accuracy did not improve from 0.87940\n",
            "391/390 [==============================] - 35s 91ms/step - loss: 1.5674 - accuracy: 0.8933 - val_loss: 1.6036 - val_accuracy: 0.8577\n",
            "Epoch 153/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5704 - accuracy: 0.8905\n",
            "Epoch 00153: val_accuracy did not improve from 0.87940\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5704 - accuracy: 0.8905 - val_loss: 1.6124 - val_accuracy: 0.8487\n",
            "Epoch 154/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5675 - accuracy: 0.8940\n",
            "Epoch 00154: val_accuracy did not improve from 0.87940\n",
            "391/390 [==============================] - 35s 91ms/step - loss: 1.5675 - accuracy: 0.8940 - val_loss: 1.5931 - val_accuracy: 0.8678\n",
            "Epoch 155/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5663 - accuracy: 0.8954\n",
            "Epoch 00155: val_accuracy improved from 0.87940 to 0.88060, saving model to model_save/weights-155-0.8806.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5663 - accuracy: 0.8954 - val_loss: 1.5806 - val_accuracy: 0.8806\n",
            "Epoch 156/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5657 - accuracy: 0.8959\n",
            "Epoch 00156: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5657 - accuracy: 0.8959 - val_loss: 1.6056 - val_accuracy: 0.8558\n",
            "Epoch 157/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5665 - accuracy: 0.8945\n",
            "Epoch 00157: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5665 - accuracy: 0.8945 - val_loss: 1.6076 - val_accuracy: 0.8526\n",
            "Epoch 158/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5659 - accuracy: 0.8954\n",
            "Epoch 00158: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5659 - accuracy: 0.8954 - val_loss: 1.5803 - val_accuracy: 0.8797\n",
            "Epoch 159/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5659 - accuracy: 0.8955\n",
            "Epoch 00159: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5659 - accuracy: 0.8955 - val_loss: 1.5817 - val_accuracy: 0.8784\n",
            "Epoch 160/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5647 - accuracy: 0.8964\n",
            "Epoch 00160: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5647 - accuracy: 0.8964 - val_loss: 1.6126 - val_accuracy: 0.8478\n",
            "Epoch 161/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5649 - accuracy: 0.8962\n",
            "Epoch 00161: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5649 - accuracy: 0.8962 - val_loss: 1.6085 - val_accuracy: 0.8526\n",
            "Epoch 162/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5649 - accuracy: 0.8968\n",
            "Epoch 00162: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5649 - accuracy: 0.8968 - val_loss: 1.5884 - val_accuracy: 0.8730\n",
            "Epoch 163/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5647 - accuracy: 0.8968\n",
            "Epoch 00163: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5647 - accuracy: 0.8968 - val_loss: 1.5959 - val_accuracy: 0.8656\n",
            "Epoch 164/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5628 - accuracy: 0.8986\n",
            "Epoch 00164: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5628 - accuracy: 0.8986 - val_loss: 1.6317 - val_accuracy: 0.8282\n",
            "Epoch 165/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5644 - accuracy: 0.8969\n",
            "Epoch 00165: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 35s 91ms/step - loss: 1.5644 - accuracy: 0.8969 - val_loss: 1.5982 - val_accuracy: 0.8622\n",
            "Epoch 166/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5646 - accuracy: 0.8966\n",
            "Epoch 00166: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 35s 91ms/step - loss: 1.5646 - accuracy: 0.8966 - val_loss: 1.5852 - val_accuracy: 0.8748\n",
            "Epoch 167/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5612 - accuracy: 0.9002\n",
            "Epoch 00167: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5612 - accuracy: 0.9002 - val_loss: 1.5885 - val_accuracy: 0.8728\n",
            "Epoch 168/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5615 - accuracy: 0.8998\n",
            "Epoch 00168: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5615 - accuracy: 0.8998 - val_loss: 1.5877 - val_accuracy: 0.8722\n",
            "Epoch 169/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5619 - accuracy: 0.8990\n",
            "Epoch 00169: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5619 - accuracy: 0.8990 - val_loss: 1.5982 - val_accuracy: 0.8617\n",
            "Epoch 170/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5632 - accuracy: 0.8979\n",
            "Epoch 00170: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5632 - accuracy: 0.8979 - val_loss: 1.5993 - val_accuracy: 0.8612\n",
            "Epoch 171/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5624 - accuracy: 0.8986\n",
            "Epoch 00171: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5624 - accuracy: 0.8986 - val_loss: 1.5877 - val_accuracy: 0.8728\n",
            "Epoch 172/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5603 - accuracy: 0.9007\n",
            "Epoch 00172: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5603 - accuracy: 0.9007 - val_loss: 1.5891 - val_accuracy: 0.8713\n",
            "Epoch 173/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5623 - accuracy: 0.8992\n",
            "Epoch 00173: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5623 - accuracy: 0.8992 - val_loss: 1.5903 - val_accuracy: 0.8700\n",
            "Epoch 174/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5602 - accuracy: 0.9014\n",
            "Epoch 00174: val_accuracy did not improve from 0.88060\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5602 - accuracy: 0.9014 - val_loss: 1.6087 - val_accuracy: 0.8518\n",
            "Epoch 175/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5590 - accuracy: 0.9025\n",
            "Epoch 00175: val_accuracy improved from 0.88060 to 0.88290, saving model to model_save/weights-175-0.8829.hdf5\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5590 - accuracy: 0.9025 - val_loss: 1.5775 - val_accuracy: 0.8829\n",
            "Epoch 176/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5592 - accuracy: 0.9026\n",
            "Epoch 00176: val_accuracy did not improve from 0.88290\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5592 - accuracy: 0.9026 - val_loss: 1.6275 - val_accuracy: 0.8334\n",
            "Epoch 177/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5583 - accuracy: 0.9030\n",
            "Epoch 00177: val_accuracy did not improve from 0.88290\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5583 - accuracy: 0.9030 - val_loss: 1.5825 - val_accuracy: 0.8790\n",
            "Epoch 178/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5585 - accuracy: 0.9030\n",
            "Epoch 00178: val_accuracy improved from 0.88290 to 0.88370, saving model to model_save/weights-178-0.8837.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5585 - accuracy: 0.9030 - val_loss: 1.5773 - val_accuracy: 0.8837\n",
            "Epoch 179/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5603 - accuracy: 0.9011\n",
            "Epoch 00179: val_accuracy did not improve from 0.88370\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5603 - accuracy: 0.9011 - val_loss: 1.5899 - val_accuracy: 0.8706\n",
            "Epoch 180/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5588 - accuracy: 0.9021\n",
            "Epoch 00180: val_accuracy did not improve from 0.88370\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5588 - accuracy: 0.9021 - val_loss: 1.5969 - val_accuracy: 0.8647\n",
            "Epoch 181/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5591 - accuracy: 0.9026\n",
            "Epoch 00181: val_accuracy improved from 0.88370 to 0.88550, saving model to model_save/weights-181-0.8855.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5591 - accuracy: 0.9026 - val_loss: 1.5752 - val_accuracy: 0.8855\n",
            "Epoch 182/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5569 - accuracy: 0.9042\n",
            "Epoch 00182: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5569 - accuracy: 0.9042 - val_loss: 1.5912 - val_accuracy: 0.8696\n",
            "Epoch 183/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5566 - accuracy: 0.9049\n",
            "Epoch 00183: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5566 - accuracy: 0.9049 - val_loss: 1.5943 - val_accuracy: 0.8665\n",
            "Epoch 184/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5584 - accuracy: 0.9031\n",
            "Epoch 00184: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5584 - accuracy: 0.9031 - val_loss: 1.6025 - val_accuracy: 0.8571\n",
            "Epoch 185/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5578 - accuracy: 0.9035\n",
            "Epoch 00185: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5578 - accuracy: 0.9035 - val_loss: 1.5951 - val_accuracy: 0.8660\n",
            "Epoch 186/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5581 - accuracy: 0.9035\n",
            "Epoch 00186: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5581 - accuracy: 0.9035 - val_loss: 1.5833 - val_accuracy: 0.8784\n",
            "Epoch 187/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5554 - accuracy: 0.9061\n",
            "Epoch 00187: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5554 - accuracy: 0.9061 - val_loss: 1.5992 - val_accuracy: 0.8603\n",
            "Epoch 188/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5573 - accuracy: 0.9042\n",
            "Epoch 00188: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5573 - accuracy: 0.9042 - val_loss: 1.5887 - val_accuracy: 0.8720\n",
            "Epoch 189/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5574 - accuracy: 0.9041\n",
            "Epoch 00189: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5574 - accuracy: 0.9041 - val_loss: 1.6083 - val_accuracy: 0.8517\n",
            "Epoch 190/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5577 - accuracy: 0.9038\n",
            "Epoch 00190: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5577 - accuracy: 0.9038 - val_loss: 1.5766 - val_accuracy: 0.8836\n",
            "Epoch 191/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.9043\n",
            "Epoch 00191: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5570 - accuracy: 0.9043 - val_loss: 1.6088 - val_accuracy: 0.8513\n",
            "Epoch 192/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5552 - accuracy: 0.9059\n",
            "Epoch 00192: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5552 - accuracy: 0.9059 - val_loss: 1.6045 - val_accuracy: 0.8566\n",
            "Epoch 193/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5552 - accuracy: 0.9063\n",
            "Epoch 00193: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5552 - accuracy: 0.9063 - val_loss: 1.6068 - val_accuracy: 0.8538\n",
            "Epoch 194/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5535 - accuracy: 0.9082\n",
            "Epoch 00194: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5535 - accuracy: 0.9082 - val_loss: 1.5759 - val_accuracy: 0.8851\n",
            "Epoch 195/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5531 - accuracy: 0.9082\n",
            "Epoch 00195: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5531 - accuracy: 0.9082 - val_loss: 1.5860 - val_accuracy: 0.8746\n",
            "Epoch 196/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5567 - accuracy: 0.9048\n",
            "Epoch 00196: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5567 - accuracy: 0.9048 - val_loss: 1.5895 - val_accuracy: 0.8709\n",
            "Epoch 197/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5507 - accuracy: 0.9112\n",
            "Epoch 00197: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5507 - accuracy: 0.9112 - val_loss: 1.5893 - val_accuracy: 0.8710\n",
            "Epoch 198/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5526 - accuracy: 0.9086\n",
            "Epoch 00198: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5526 - accuracy: 0.9086 - val_loss: 1.5840 - val_accuracy: 0.8770\n",
            "Epoch 199/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5537 - accuracy: 0.9075\n",
            "Epoch 00199: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5537 - accuracy: 0.9075 - val_loss: 1.5807 - val_accuracy: 0.8807\n",
            "Epoch 200/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5534 - accuracy: 0.9083\n",
            "Epoch 00200: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5534 - accuracy: 0.9083 - val_loss: 1.5839 - val_accuracy: 0.8762\n",
            "Epoch 201/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5542 - accuracy: 0.9073\n",
            "Epoch 00201: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5542 - accuracy: 0.9073 - val_loss: 1.5807 - val_accuracy: 0.8804\n",
            "Epoch 202/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5540 - accuracy: 0.9073\n",
            "Epoch 00202: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5540 - accuracy: 0.9073 - val_loss: 1.6170 - val_accuracy: 0.8426\n",
            "Epoch 203/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5529 - accuracy: 0.9085\n",
            "Epoch 00203: val_accuracy did not improve from 0.88550\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5529 - accuracy: 0.9085 - val_loss: 1.5774 - val_accuracy: 0.8829\n",
            "Epoch 204/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5518 - accuracy: 0.9097\n",
            "Epoch 00204: val_accuracy improved from 0.88550 to 0.88810, saving model to model_save/weights-204-0.8881.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5518 - accuracy: 0.9097 - val_loss: 1.5722 - val_accuracy: 0.8881\n",
            "Epoch 205/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5517 - accuracy: 0.9097\n",
            "Epoch 00205: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5517 - accuracy: 0.9097 - val_loss: 1.5927 - val_accuracy: 0.8680\n",
            "Epoch 206/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5521 - accuracy: 0.9091\n",
            "Epoch 00206: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5521 - accuracy: 0.9091 - val_loss: 1.5873 - val_accuracy: 0.8731\n",
            "Epoch 207/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5508 - accuracy: 0.9101\n",
            "Epoch 00207: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5508 - accuracy: 0.9101 - val_loss: 1.5729 - val_accuracy: 0.8875\n",
            "Epoch 208/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5505 - accuracy: 0.9108\n",
            "Epoch 00208: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5505 - accuracy: 0.9108 - val_loss: 1.5742 - val_accuracy: 0.8869\n",
            "Epoch 209/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5502 - accuracy: 0.9108\n",
            "Epoch 00209: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5502 - accuracy: 0.9108 - val_loss: 1.6059 - val_accuracy: 0.8542\n",
            "Epoch 210/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5510 - accuracy: 0.9106\n",
            "Epoch 00210: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5510 - accuracy: 0.9106 - val_loss: 1.5811 - val_accuracy: 0.8792\n",
            "Epoch 211/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5517 - accuracy: 0.9093\n",
            "Epoch 00211: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5517 - accuracy: 0.9093 - val_loss: 1.6192 - val_accuracy: 0.8414\n",
            "Epoch 212/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5503 - accuracy: 0.9106\n",
            "Epoch 00212: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5503 - accuracy: 0.9106 - val_loss: 1.6080 - val_accuracy: 0.8529\n",
            "Epoch 213/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5500 - accuracy: 0.9108\n",
            "Epoch 00213: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5500 - accuracy: 0.9108 - val_loss: 1.5789 - val_accuracy: 0.8814\n",
            "Epoch 214/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5514 - accuracy: 0.9102\n",
            "Epoch 00214: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5514 - accuracy: 0.9102 - val_loss: 1.5902 - val_accuracy: 0.8704\n",
            "Epoch 215/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5495 - accuracy: 0.9118\n",
            "Epoch 00215: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5495 - accuracy: 0.9118 - val_loss: 1.5814 - val_accuracy: 0.8791\n",
            "Epoch 216/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5499 - accuracy: 0.9112\n",
            "Epoch 00216: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5499 - accuracy: 0.9112 - val_loss: 1.5843 - val_accuracy: 0.8760\n",
            "Epoch 217/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5501 - accuracy: 0.9116\n",
            "Epoch 00217: val_accuracy did not improve from 0.88810\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5501 - accuracy: 0.9116 - val_loss: 1.5801 - val_accuracy: 0.8794\n",
            "Epoch 218/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5479 - accuracy: 0.9131\n",
            "Epoch 00218: val_accuracy improved from 0.88810 to 0.89320, saving model to model_save/weights-218-0.8932.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5479 - accuracy: 0.9131 - val_loss: 1.5676 - val_accuracy: 0.8932\n",
            "Epoch 219/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5499 - accuracy: 0.9112\n",
            "Epoch 00219: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5499 - accuracy: 0.9112 - val_loss: 1.5968 - val_accuracy: 0.8635\n",
            "Epoch 220/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5475 - accuracy: 0.9140\n",
            "Epoch 00220: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5475 - accuracy: 0.9140 - val_loss: 1.5796 - val_accuracy: 0.8814\n",
            "Epoch 221/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5482 - accuracy: 0.9131\n",
            "Epoch 00221: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5482 - accuracy: 0.9131 - val_loss: 1.5845 - val_accuracy: 0.8763\n",
            "Epoch 222/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5473 - accuracy: 0.9142\n",
            "Epoch 00222: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5473 - accuracy: 0.9142 - val_loss: 1.5861 - val_accuracy: 0.8749\n",
            "Epoch 223/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5484 - accuracy: 0.9127\n",
            "Epoch 00223: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5484 - accuracy: 0.9127 - val_loss: 1.5834 - val_accuracy: 0.8778\n",
            "Epoch 224/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5466 - accuracy: 0.9146\n",
            "Epoch 00224: val_accuracy did not improve from 0.89320\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5466 - accuracy: 0.9146 - val_loss: 1.5788 - val_accuracy: 0.8816\n",
            "Epoch 225/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5471 - accuracy: 0.9145\n",
            "Epoch 00225: val_accuracy improved from 0.89320 to 0.89560, saving model to model_save/weights-225-0.8956.hdf5\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5471 - accuracy: 0.9145 - val_loss: 1.5649 - val_accuracy: 0.8956\n",
            "Epoch 226/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5458 - accuracy: 0.9155\n",
            "Epoch 00226: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5458 - accuracy: 0.9155 - val_loss: 1.5849 - val_accuracy: 0.8762\n",
            "Epoch 227/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5440 - accuracy: 0.9175\n",
            "Epoch 00227: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5440 - accuracy: 0.9175 - val_loss: 1.5931 - val_accuracy: 0.8679\n",
            "Epoch 228/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5470 - accuracy: 0.9143\n",
            "Epoch 00228: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5470 - accuracy: 0.9143 - val_loss: 1.5680 - val_accuracy: 0.8924\n",
            "Epoch 229/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5443 - accuracy: 0.9174\n",
            "Epoch 00229: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5443 - accuracy: 0.9174 - val_loss: 1.5679 - val_accuracy: 0.8927\n",
            "Epoch 230/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5467 - accuracy: 0.9148\n",
            "Epoch 00230: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5467 - accuracy: 0.9148 - val_loss: 1.5833 - val_accuracy: 0.8783\n",
            "Epoch 231/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5449 - accuracy: 0.9161\n",
            "Epoch 00231: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5449 - accuracy: 0.9161 - val_loss: 1.5768 - val_accuracy: 0.8833\n",
            "Epoch 232/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5459 - accuracy: 0.9157\n",
            "Epoch 00232: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5459 - accuracy: 0.9157 - val_loss: 1.6100 - val_accuracy: 0.8512\n",
            "Epoch 233/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5467 - accuracy: 0.9142\n",
            "Epoch 00233: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 37s 95ms/step - loss: 1.5467 - accuracy: 0.9142 - val_loss: 1.5782 - val_accuracy: 0.8827\n",
            "Epoch 234/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5439 - accuracy: 0.9174\n",
            "Epoch 00234: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5439 - accuracy: 0.9174 - val_loss: 1.5727 - val_accuracy: 0.8882\n",
            "Epoch 235/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5462 - accuracy: 0.9154\n",
            "Epoch 00235: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5462 - accuracy: 0.9154 - val_loss: 1.5677 - val_accuracy: 0.8930\n",
            "Epoch 236/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5430 - accuracy: 0.9181\n",
            "Epoch 00236: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5430 - accuracy: 0.9181 - val_loss: 1.5664 - val_accuracy: 0.8939\n",
            "Epoch 237/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5447 - accuracy: 0.9168\n",
            "Epoch 00237: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5447 - accuracy: 0.9168 - val_loss: 1.5942 - val_accuracy: 0.8659\n",
            "Epoch 238/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5452 - accuracy: 0.9164\n",
            "Epoch 00238: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5452 - accuracy: 0.9164 - val_loss: 1.5776 - val_accuracy: 0.8830\n",
            "Epoch 239/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5435 - accuracy: 0.9178\n",
            "Epoch 00239: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5435 - accuracy: 0.9178 - val_loss: 1.5763 - val_accuracy: 0.8854\n",
            "Epoch 240/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5434 - accuracy: 0.9180\n",
            "Epoch 00240: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5434 - accuracy: 0.9180 - val_loss: 1.5795 - val_accuracy: 0.8811\n",
            "Epoch 241/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5442 - accuracy: 0.9173\n",
            "Epoch 00241: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5442 - accuracy: 0.9173 - val_loss: 1.5690 - val_accuracy: 0.8917\n",
            "Epoch 242/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5415 - accuracy: 0.9199\n",
            "Epoch 00242: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5415 - accuracy: 0.9199 - val_loss: 1.5696 - val_accuracy: 0.8920\n",
            "Epoch 243/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5422 - accuracy: 0.9194\n",
            "Epoch 00243: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5422 - accuracy: 0.9194 - val_loss: 1.5964 - val_accuracy: 0.8649\n",
            "Epoch 244/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5446 - accuracy: 0.9166\n",
            "Epoch 00244: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5446 - accuracy: 0.9166 - val_loss: 1.5832 - val_accuracy: 0.8768\n",
            "Epoch 245/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5445 - accuracy: 0.9168\n",
            "Epoch 00245: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5445 - accuracy: 0.9168 - val_loss: 1.5782 - val_accuracy: 0.8828\n",
            "Epoch 246/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5436 - accuracy: 0.9177\n",
            "Epoch 00246: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5436 - accuracy: 0.9177 - val_loss: 1.5860 - val_accuracy: 0.8755\n",
            "Epoch 247/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5435 - accuracy: 0.9176\n",
            "Epoch 00247: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5435 - accuracy: 0.9176 - val_loss: 1.5781 - val_accuracy: 0.8828\n",
            "Epoch 248/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5414 - accuracy: 0.9196\n",
            "Epoch 00248: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5414 - accuracy: 0.9196 - val_loss: 1.5738 - val_accuracy: 0.8867\n",
            "Epoch 249/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5416 - accuracy: 0.9203\n",
            "Epoch 00249: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5416 - accuracy: 0.9203 - val_loss: 1.5741 - val_accuracy: 0.8872\n",
            "Epoch 250/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5429 - accuracy: 0.9180\n",
            "Epoch 00250: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5429 - accuracy: 0.9180 - val_loss: 1.5694 - val_accuracy: 0.8912\n",
            "Epoch 251/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5422 - accuracy: 0.9196\n",
            "Epoch 00251: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5422 - accuracy: 0.9196 - val_loss: 1.5880 - val_accuracy: 0.8730\n",
            "Epoch 252/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5426 - accuracy: 0.9184\n",
            "Epoch 00252: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5426 - accuracy: 0.9184 - val_loss: 1.5694 - val_accuracy: 0.8909\n",
            "Epoch 253/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5404 - accuracy: 0.9215\n",
            "Epoch 00253: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5404 - accuracy: 0.9215 - val_loss: 1.5705 - val_accuracy: 0.8901\n",
            "Epoch 254/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5417 - accuracy: 0.9196\n",
            "Epoch 00254: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5417 - accuracy: 0.9196 - val_loss: 1.5837 - val_accuracy: 0.8761\n",
            "Epoch 255/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5396 - accuracy: 0.9222\n",
            "Epoch 00255: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5396 - accuracy: 0.9222 - val_loss: 1.5706 - val_accuracy: 0.8897\n",
            "Epoch 256/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5423 - accuracy: 0.9191\n",
            "Epoch 00256: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5423 - accuracy: 0.9191 - val_loss: 1.5736 - val_accuracy: 0.8876\n",
            "Epoch 257/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5433 - accuracy: 0.9184\n",
            "Epoch 00257: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5433 - accuracy: 0.9184 - val_loss: 1.5688 - val_accuracy: 0.8914\n",
            "Epoch 258/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5405 - accuracy: 0.9210\n",
            "Epoch 00258: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5405 - accuracy: 0.9210 - val_loss: 1.5707 - val_accuracy: 0.8903\n",
            "Epoch 259/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5405 - accuracy: 0.9209\n",
            "Epoch 00259: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5405 - accuracy: 0.9209 - val_loss: 1.5830 - val_accuracy: 0.8775\n",
            "Epoch 260/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5396 - accuracy: 0.9218\n",
            "Epoch 00260: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5396 - accuracy: 0.9218 - val_loss: 1.5844 - val_accuracy: 0.8764\n",
            "Epoch 261/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5389 - accuracy: 0.9225\n",
            "Epoch 00261: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5389 - accuracy: 0.9225 - val_loss: 1.5867 - val_accuracy: 0.8723\n",
            "Epoch 262/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5407 - accuracy: 0.9208\n",
            "Epoch 00262: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5407 - accuracy: 0.9208 - val_loss: 1.5752 - val_accuracy: 0.8857\n",
            "Epoch 263/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5380 - accuracy: 0.9229\n",
            "Epoch 00263: val_accuracy did not improve from 0.89560\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5380 - accuracy: 0.9229 - val_loss: 1.5808 - val_accuracy: 0.8802\n",
            "Epoch 264/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5386 - accuracy: 0.9227\n",
            "Epoch 00264: val_accuracy improved from 0.89560 to 0.89630, saving model to model_save/weights-264-0.8963.hdf5\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5386 - accuracy: 0.9227 - val_loss: 1.5644 - val_accuracy: 0.8963\n",
            "Epoch 265/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5387 - accuracy: 0.9221\n",
            "Epoch 00265: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5387 - accuracy: 0.9221 - val_loss: 1.5692 - val_accuracy: 0.8923\n",
            "Epoch 266/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5403 - accuracy: 0.9212\n",
            "Epoch 00266: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5403 - accuracy: 0.9212 - val_loss: 1.5716 - val_accuracy: 0.8896\n",
            "Epoch 267/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5395 - accuracy: 0.9220\n",
            "Epoch 00267: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5395 - accuracy: 0.9220 - val_loss: 1.5704 - val_accuracy: 0.8908\n",
            "Epoch 268/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5386 - accuracy: 0.9235\n",
            "Epoch 00268: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5386 - accuracy: 0.9235 - val_loss: 1.5742 - val_accuracy: 0.8861\n",
            "Epoch 269/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5368 - accuracy: 0.9250\n",
            "Epoch 00269: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5368 - accuracy: 0.9250 - val_loss: 1.5773 - val_accuracy: 0.8836\n",
            "Epoch 270/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5360 - accuracy: 0.9258\n",
            "Epoch 00270: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5360 - accuracy: 0.9258 - val_loss: 1.5708 - val_accuracy: 0.8893\n",
            "Epoch 271/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5384 - accuracy: 0.9230\n",
            "Epoch 00271: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5384 - accuracy: 0.9230 - val_loss: 1.5841 - val_accuracy: 0.8772\n",
            "Epoch 272/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5372 - accuracy: 0.9242\n",
            "Epoch 00272: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5372 - accuracy: 0.9242 - val_loss: 1.5774 - val_accuracy: 0.8830\n",
            "Epoch 273/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5347 - accuracy: 0.9268\n",
            "Epoch 00273: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5347 - accuracy: 0.9268 - val_loss: 1.5684 - val_accuracy: 0.8921\n",
            "Epoch 274/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5349 - accuracy: 0.9273\n",
            "Epoch 00274: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5349 - accuracy: 0.9273 - val_loss: 1.5864 - val_accuracy: 0.8742\n",
            "Epoch 275/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5350 - accuracy: 0.9264\n",
            "Epoch 00275: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5350 - accuracy: 0.9264 - val_loss: 1.5870 - val_accuracy: 0.8738\n",
            "Epoch 276/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.9242\n",
            "Epoch 00276: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5374 - accuracy: 0.9242 - val_loss: 1.5661 - val_accuracy: 0.8948\n",
            "Epoch 277/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.9241\n",
            "Epoch 00277: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5374 - accuracy: 0.9241 - val_loss: 1.5760 - val_accuracy: 0.8850\n",
            "Epoch 278/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5379 - accuracy: 0.9232\n",
            "Epoch 00278: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5379 - accuracy: 0.9232 - val_loss: 1.5751 - val_accuracy: 0.8852\n",
            "Epoch 279/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5366 - accuracy: 0.9249\n",
            "Epoch 00279: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5366 - accuracy: 0.9249 - val_loss: 1.5677 - val_accuracy: 0.8921\n",
            "Epoch 280/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5379 - accuracy: 0.9233\n",
            "Epoch 00280: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5379 - accuracy: 0.9233 - val_loss: 1.5671 - val_accuracy: 0.8944\n",
            "Epoch 281/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5365 - accuracy: 0.9248\n",
            "Epoch 00281: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5365 - accuracy: 0.9248 - val_loss: 1.5654 - val_accuracy: 0.8956\n",
            "Epoch 282/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5348 - accuracy: 0.9267\n",
            "Epoch 00282: val_accuracy did not improve from 0.89630\n",
            "391/390 [==============================] - 36s 91ms/step - loss: 1.5348 - accuracy: 0.9267 - val_loss: 1.5719 - val_accuracy: 0.8899\n",
            "Epoch 283/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5365 - accuracy: 0.9246\n",
            "Epoch 00283: val_accuracy improved from 0.89630 to 0.90440, saving model to model_save/weights-283-0.9044.hdf5\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5365 - accuracy: 0.9246 - val_loss: 1.5559 - val_accuracy: 0.9044\n",
            "Epoch 284/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5347 - accuracy: 0.9271\n",
            "Epoch 00284: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5347 - accuracy: 0.9271 - val_loss: 1.5842 - val_accuracy: 0.8763\n",
            "Epoch 285/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5363 - accuracy: 0.9252\n",
            "Epoch 00285: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5363 - accuracy: 0.9252 - val_loss: 1.5727 - val_accuracy: 0.8875\n",
            "Epoch 286/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5346 - accuracy: 0.9266\n",
            "Epoch 00286: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5346 - accuracy: 0.9266 - val_loss: 1.5727 - val_accuracy: 0.8879\n",
            "Epoch 287/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5356 - accuracy: 0.9258\n",
            "Epoch 00287: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5356 - accuracy: 0.9258 - val_loss: 1.5582 - val_accuracy: 0.9022\n",
            "Epoch 288/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5348 - accuracy: 0.9264\n",
            "Epoch 00288: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5348 - accuracy: 0.9264 - val_loss: 1.5615 - val_accuracy: 0.8991\n",
            "Epoch 289/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5339 - accuracy: 0.9278\n",
            "Epoch 00289: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5339 - accuracy: 0.9278 - val_loss: 1.5918 - val_accuracy: 0.8696\n",
            "Epoch 290/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5342 - accuracy: 0.9276\n",
            "Epoch 00290: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5342 - accuracy: 0.9276 - val_loss: 1.5704 - val_accuracy: 0.8902\n",
            "Epoch 291/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5339 - accuracy: 0.9271\n",
            "Epoch 00291: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5339 - accuracy: 0.9271 - val_loss: 1.5641 - val_accuracy: 0.8958\n",
            "Epoch 292/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5333 - accuracy: 0.9284\n",
            "Epoch 00292: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 92ms/step - loss: 1.5333 - accuracy: 0.9284 - val_loss: 1.5687 - val_accuracy: 0.8918\n",
            "Epoch 293/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5343 - accuracy: 0.9273\n",
            "Epoch 00293: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5343 - accuracy: 0.9273 - val_loss: 1.5886 - val_accuracy: 0.8720\n",
            "Epoch 294/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.9280\n",
            "Epoch 00294: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5337 - accuracy: 0.9280 - val_loss: 1.5809 - val_accuracy: 0.8785\n",
            "Epoch 295/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5354 - accuracy: 0.9257\n",
            "Epoch 00295: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5354 - accuracy: 0.9257 - val_loss: 1.5717 - val_accuracy: 0.8885\n",
            "Epoch 296/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.9291\n",
            "Epoch 00296: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5321 - accuracy: 0.9291 - val_loss: 1.5723 - val_accuracy: 0.8881\n",
            "Epoch 297/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.9277\n",
            "Epoch 00297: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5337 - accuracy: 0.9277 - val_loss: 1.5671 - val_accuracy: 0.8927\n",
            "Epoch 298/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5348 - accuracy: 0.9264\n",
            "Epoch 00298: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 36s 93ms/step - loss: 1.5348 - accuracy: 0.9264 - val_loss: 1.5757 - val_accuracy: 0.8843\n",
            "Epoch 299/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5352 - accuracy: 0.9261\n",
            "Epoch 00299: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 94ms/step - loss: 1.5352 - accuracy: 0.9261 - val_loss: 1.5598 - val_accuracy: 0.8998\n",
            "Epoch 300/300\n",
            "391/390 [==============================] - ETA: 0s - loss: 1.5336 - accuracy: 0.9275\n",
            "Epoch 00300: val_accuracy did not improve from 0.90440\n",
            "391/390 [==============================] - 37s 93ms/step - loss: 1.5336 - accuracy: 0.9275 - val_loss: 1.5726 - val_accuracy: 0.8884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d40101f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}